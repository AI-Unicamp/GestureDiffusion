{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826fc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mdm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from wavlm.WavLM import WavLM, WavLMConfig\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f7439",
   "metadata": {},
   "source": [
    "# Generate WavLM Representations and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed0784",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43eaece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavlm_layer = 11 \n",
    "fps=30\n",
    "sample_rate=16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012b799",
   "metadata": {},
   "source": [
    "## Data Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51be5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wavlm_reps_folder = 'dataset/Genea2023/trn/main-agent/wavlm_representations/'\n",
    "val_wavlm_reps_folder = 'dataset/Genea2023/val/main-agent/wavlm_representations/'\n",
    "train_audios_folder = 'dataset/Genea2023/trn/main-agent/audio16k_npy/' \n",
    "val_audios_folder = 'dataset/Genea2023/val/main-agent/audio16k_npy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c684ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_wavlm_reps_folder):\n",
    "    os.makedirs(train_wavlm_reps_folder)\n",
    "if not os.path.exists(val_wavlm_reps_folder):\n",
    "    os.makedirs(val_wavlm_reps_folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f49cce",
   "metadata": {},
   "source": [
    "## Load Pre-Trained WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860df049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WavLM(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        (3): GELU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (relative_attention_bias): Embedding(320, 12)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./wavlm/WavLM-Base+.pt')\n",
    "wavlm_cfg = WavLMConfig(checkpoint['cfg'])\n",
    "wavlm = WavLM(wavlm_cfg)\n",
    "wavlm.load_state_dict(checkpoint['model'])\n",
    "wavlm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3fd68",
   "metadata": {},
   "source": [
    "## Train Audio Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44355ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69/69 [1:39:18<00:00, 86.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(range(303, len(os.listdir(train_audios_folder)))):\n",
    "    \n",
    "    # Get Audio File\n",
    "    audio_path = train_audios_folder + os.listdir(train_audios_folder)[idx]\n",
    "    \n",
    "    # Load with Numpy\n",
    "    signal = np.load(audio_path)\n",
    "    \n",
    "    # Set to model innput format\n",
    "    signal = torch.tensor(signal).unsqueeze(0)\n",
    "    \n",
    "    # Normalize\n",
    "    if wavlm_cfg.normalize:\n",
    "        signal_norm = torch.nn.functional.layer_norm(signal , signal.shape)\n",
    "    else:\n",
    "        signal_norm = signal\n",
    "        \n",
    "    # Run Model (rep=Desired Layer, layer_results=all layers)\n",
    "    rep, layer_results = wavlm.extract_features(signal_norm, output_layer=wavlm_layer, ret_layer_results=True)[0]\n",
    "    layer_reps = [x.transpose(0, 1) for x, _ in layer_results] # fix shape\n",
    "\n",
    "    # Get Number of Seconds of Audio File\n",
    "    n_secs = signal.shape[1] / sample_rate\n",
    "    \n",
    "    # Get Number of poses equivalent to audio file duration, given fps (alignment len)\n",
    "    n_pose = n_secs * fps\n",
    "    \n",
    "    # Interpolate number of representations to match number of poses corresponding to audio file\n",
    "    interp_reps = F.interpolate(rep.transpose(1, 2), size=int(n_pose), align_corners=True, mode='linear')\n",
    "    \n",
    "    # Prepare to save\n",
    "    interp_reps = interp_reps.squeeze(0).transpose(0,1).cpu().detach().data.cpu().numpy()\n",
    "    \n",
    "    # Double check dimension\n",
    "    assert (interp_reps.shape[0] == int(np.ceil(n_pose)) or interp_reps.shape[0] == int(np.floor(n_pose)))\n",
    "    \n",
    "    #Save\n",
    "    path_name = train_wavlm_reps_folder + os.listdir(train_audios_folder)[idx]\n",
    "    with open(path_name, 'wb') as f:\n",
    "        np.save(f, interp_reps)\n",
    "    \n",
    "    # Normalize OutBatch of Desired Layer\n",
    "    rep_norm = wavlm.encoder.layer_norm(rep)\n",
    "    \n",
    "    # Interpolate number of representations to match number of poses corresponding to audio file\n",
    "    interp_reps_norm = F.interpolate(rep_norm.transpose(1, 2), size=int(n_pose), align_corners=True, mode='linear')\n",
    "    \n",
    "    # Prepare to save\n",
    "    interp_reps_norm = interp_reps_norm.squeeze(0).transpose(0,1).cpu().detach().data.cpu().numpy()\n",
    "    \n",
    "    # Double check dimension\n",
    "    assert (interp_reps_norm.shape[0] == int(np.ceil(n_pose)) or interp_reps_norm.shape[0] == int(np.floor(n_pose)))\n",
    "    \n",
    "    #Save\n",
    "    path_name = train_wavlm_reps_folder + 'norm_' + os.listdir(train_audios_folder)[idx]\n",
    "    with open(path_name, 'wb') as f:\n",
    "        np.save(f, interp_reps_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f48c57",
   "metadata": {},
   "source": [
    "## Val Audio Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501a264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [10:22<00:00, 15.19s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(range(len(os.listdir(val_audios_folder)))):\n",
    "    \n",
    "    # Get Audio File\n",
    "    audio_path = val_audios_folder + os.listdir(val_audios_folder)[idx]\n",
    "    \n",
    "    # Load with Numpy\n",
    "    signal = np.load(audio_path)\n",
    "    \n",
    "    # Set to model innput format\n",
    "    signal = torch.tensor(signal).unsqueeze(0)\n",
    "    \n",
    "    # Normalize\n",
    "    if wavlm_cfg.normalize:\n",
    "        signal_norm = torch.nn.functional.layer_norm(signal , signal.shape)\n",
    "    else:\n",
    "        signal_norm = signal\n",
    "        \n",
    "    # Run Model (rep=Desired Layer, layer_results=all layers)\n",
    "    rep, layer_results = wavlm.extract_features(signal_norm, output_layer=wavlm_layer, ret_layer_results=True)[0]\n",
    "    layer_reps = [x.transpose(0, 1) for x, _ in layer_results] # fix shape\n",
    "\n",
    "    # Get Number of Seconds of Audio File\n",
    "    n_secs = signal.shape[1] / sample_rate\n",
    "    \n",
    "    # Get Number of poses equivalent to audio file duration, given fps (alignment len)\n",
    "    n_pose = n_secs * fps\n",
    "    \n",
    "    # Interpolate number of representations to match number of poses corresponding to audio file\n",
    "    interp_reps = F.interpolate(rep.transpose(1, 2), size=int(n_pose), align_corners=True, mode='linear')\n",
    "    \n",
    "    # Prepare to save\n",
    "    interp_reps = interp_reps.squeeze(0).transpose(0,1).cpu().detach().data.cpu().numpy()\n",
    "    \n",
    "    # Double check dimension\n",
    "    assert (interp_reps.shape[0] == int(np.ceil(n_pose)) or interp_reps.shape[0] == int(np.floor(n_pose)))\n",
    "    \n",
    "    #Save\n",
    "    path_name = val_wavlm_reps_folder + os.listdir(val_audios_folder)[idx]\n",
    "    with open(path_name, 'wb') as f:\n",
    "        np.save(f, interp_reps)\n",
    "    \n",
    "    # Normalize OutBatch of Desired Layer\n",
    "    rep_norm = wavlm.encoder.layer_norm(rep)\n",
    "    \n",
    "    # Interpolate number of representations to match number of poses corresponding to audio file\n",
    "    interp_reps_norm = F.interpolate(rep_norm.transpose(1, 2), size=int(n_pose), align_corners=True, mode='linear')\n",
    "    \n",
    "    # Prepare to save\n",
    "    interp_reps_norm = interp_reps_norm.squeeze(0).transpose(0,1).cpu().detach().data.cpu().numpy()\n",
    "    \n",
    "    # Double check dimension\n",
    "    assert (interp_reps_norm.shape[0] == int(np.ceil(n_pose)) or interp_reps_norm.shape[0] == int(np.floor(n_pose)))\n",
    "    \n",
    "    #Save\n",
    "    path_name = val_wavlm_reps_folder + 'norm_' + os.listdir(val_audios_folder)[idx]\n",
    "    with open(path_name, 'wb') as f:\n",
    "        np.save(f, interp_reps_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157fd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
